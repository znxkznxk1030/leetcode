# 중간고사 예상 문제

## Introduction

### 1. 다음 개념들: 인공지능(AI), 머신러닝(ML), 딥러닝 (DL)의 관계를 설명하고, 각각 대표적인 예시를 하나씩 들어 서술하시오

$$ 딥러닝 \subset 머신러닝 \subset 인공지능$$

```txt
인공지능은 인간처럼 사고하고 행동하는 기계를 만드는 기술 전반을 의미한다.
머신러닝은 AI의 한 분야로, 데이터를 기반으로 스스로 학습하고 성능을 개선하는 알고리즘을 의미한다.
딥러닝은 머신러닝의 하위 개념으로 인공신경망(특히 다중 신경망)을 이용하여 복잡한 문제를 해결한다.
```

- AI: 체스 두는 인공지능
- ML: 이메일 스펨 분류기
- DL: 자율주행차의 영상 인식 모델 (CNN)

## Linear Regression

### 1. Linear Regression의 정의

```txt
하나 이상의 독립 변수(x)를 사용해서 종속 변수(y)를 예측하는 모델.
```

- 간단한 linear regression 모델
$$ f(x) = w_0 + w_1x$$

### 2. RSS(Residual Sum of Squares)의 정의를 서술하고, 이것이 선형 회귀에서 어떤 역할을 하는지 설명하시오

$$ RSS = \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 $$

```txt
RSS는 각 데이터 포인트의 실제 값과 예측 값간의 잔차(residual)를 제곱하여 모두 더한 값이다.
RSS는 회귀 모델의 예측 오차의 크기를 나타내며, RSS가 작을 수록 모델이 데이터를 더 잘 설명한다.
```

### 3. 다음 데이터에 대해 단순선형회귀 모델 $ŷ = 2 + 1.5x$를 적합했습니다

| x| y|
|:-----:|:-----:|
| 1   | 4.0   |
| 2   | 5.5   |
| 3   | 8.0   |

- RSS를 계산하시오.

$$RSS = \sum_{i = 1}^{n}(y_i - \hat{y}_i)^2 = (4.0 - 3.5)^2 + (5.5 - 5.0)^2 + (8.0 - 6.5)^2 = 2.75$$

### 4. 다항 회귀에서 고차항을 추가해도 선형 회귀 모델의 일종인 이유는 무엇인가?

```txt
입력 변수 x의 형태가 비선형일지라도, 파라미터 w에 대해 선형이므로 다항 회귀도 선형 모델이다.
```

$$y_i = w_0h_0(x_i) + w_1h_1(x_i) + ... + w_Dh_D(x_i) + \epsilon_i = \sum_{j=0}^{D}w_jh_j(x_i) + \epsilon_i$$

### 5. 선형 회귀 모델 $\hat{y} = H\mathbf{w}$에서 RSS를 최소화 하기 위한 $\hat{w}$의 Closed-form 해를 유도하시오

- $$ RSS(w) = ||y - Hw||^2 = (y - Hw)^T(y - Hw) = y^Ty - y^T(Hw) - (Hw)^Ty +(Hw)^THw \\= y^Ty - 2w^TH^Ty + w^TH^THw$$

- $$\nabla RSS(w) = -2H^Ty + 2H^THw = -2H^T(y - Hw)$$

- $$\nabla RSS(w) = 0 \to -2H^Ty + 2H^THw = 0  \to H^THw = H^Ty$$
- $$ \therefore w = (H^TH)^{-1}H^Ty$$

> - $ab = b^Ta^T$
> - $f(w) = Aw = w^TA$ -> $f'(w) = A$
> - $g(w) = w^TAw$ -> $g'(w) = 2Aw$

### 6. 선형 회귀에서 경사하강법은 항상 전역 최솟값을 찾을 수 있다. (O/X)

```txt
O (정답이다.)
```

선형 함수에서 사용하는 손실함수 (RSS, MSE)는 이차함수이다.\
이차함수는 볼록 함수(Convex)이므로 최소값이 하나 뿐이다.\
그러므로 이론적으로는 gradient descent는 전역 최적해를 찾을 수 있다.\
단, 학습률 (learning rate)가 너무 크면 발산할 수도 있으므로, 적절한 $\eta$를 설정해야한다.

## Logistic Regression

### 1. 선형 회귀로 이진 분류 분제를 풀었을 때 발생할 수 있는 문제점을 서술하고, 이러한 문제를 해결하기 위해 로지스틱 회귀(Logistic Regression)가 도입된 이유를 설명하시오

```txt
선형 회귀는 연속적인 실수값을 예측하기 때문에 이진 분류 문제에 사용할 경우, 출력값이 0보다 작거나 1보다 큰 값이 나올 수 있다.
이는 확률로 해석할 수 없고, 임계값(threshold)을 인위적으로 정해야 하는 문제가 있다.
```

```txt
로지스틱 회귀는 출력값을 0과 1 사이의 값으로 제한하기 위해 시그모이드 함수를 사용한다.
이를 통해 예측값을 확률로 해석할 수 있으며, 분류 문제에 적합하다.
```

### 2. 최대 우도 추정(MLE)의 개념을 서술하시오. 또한 주어진 데이터가 어떤 확률 분포로부터 생성되었다고 가정할 때, MLE가 어떤 방식으로 그 분포의 파라미터를 추정하는지 설명하시오

```txt
MLE는 관측된 데이터를 가장 잘 설명하는 확률 분포의 파라미터를 찾는 방법이다.
우도 함수(likelihood function)는 데이터가 주어졌을 때 파라미터 하에서 데이터가 발생할 확률을 의미한다.
MLE는 이 우도 함수를 최대화하는 파라미터 값을 찾는 것을 목표로 한다.
```

### 3. 최대 우도 추정(MLE)에서 Log-likelihood를 사용하는 이유를 서술하시오

1. 곱을 합으로 바꾸기 위해
2. 수치적 안정성

단조 증가 함수 이므로 최적값이 동일하다.

$$argmax_\theta L(\theta) = argmax_\theta logL(\theta)$$

### 4. 이진 분류에 대한 Log-likelihood $ll(w)$가 주어졌을 때, $\frac{\partial ll(w)}{ \partial w_j}$를 구하시오

### 5. NLL(Negative Log-Likelihood)와 Cross Entropy의 관계

Negative Log-Likelihood는 주어진 정답 클래스에 대해 모델이 예측한 확률의 로그값에 음수를 취한 손실 함수이다.\
예측 확률이 높을수록 손실은 작아지고, 낮을수록 손실이 커진다.

$$NLL = -log (p(y))$$

Cross Entropy는 두 확률 분포 사이의 차이를 측정하는 지표로, 일반적으로 정답 레이블이 원-핫 인코딩 되어 있을 때 다음과 같이 정의 된다.

$$ Cross\space Entropy = -\sum_i y_i log(p_i)$$

이때, $y_i$는 정답 분포, $p_i$는 모델의 예측 분포이다.\
정답이 원-핫 벡터일 경우, 오직 정답 클래스에 대해서만 $y_i = 1$이고 나머지는 0이므로 Cross Entropy는 NLL과 동일한 형태로 계산된다.

결론적으로, 분류 문제에서 Cross Entropy는 NLL과 같은 방식으로 작동하며,\
특히 정답 레이블이 원-핫일 경우 두 손실 함수는 사실상 동일하다.

### 6. 다중 클래스 분류 문제에서 사용되는 softmax함수에 대해 설명하시오. softmax 함수의 수식을 쓰고, 이 함수가 출력 값에 어떤 역할을 하는지 기술하시오

Softmax 함수는 다중 클래스 분류 문제에서 출력층에 사용되는 활성화 함수로, 각 클래스에 대한 점수를 확률로 변환하는 역할을 한다.\
이 함수는 모델의 출력값을 정규화하여 모든 클래스에 대한 예측 확률의 합이 1이 되도록 만든다.

$$P(y=c|x, w_1, ..., w_C)= \frac{e^{w_c^Th(x)}}{\sum_i^C e^{w_c^Th(x)}}$$

이 함수를 사용하면, 로짓 값이 클수록 해당 클래스에 대한 확률이 높게 할당되며,\
모든 클래스에 대한 상대적인 비교를 통해 분류 결과를 해석할 수 있게 된다.\
또한 Softmax는 출력값을 확률 분포로 바꾸기 때문에 Cross Entropy 손실 함수와 함께 자주 사용된다.

## Overfitting and regularization

### 1. 선형 회귀(Linear Regression)에서 Overfitting이 발생하는 원인을 설명하고, 이를 해결하기 위한 방법들을 서술하시오. 각각의 방법이 Overfitting을 완화하는 이유도 함께 기술하시오

Overfitting이란 모델이 학습 데이터에 과도하게 적합하여, 새로운 데이터(테스트 데이터)에 대한 일반화 성능이 떨어지는 현상이다. 선형 회귀에서는 특히 다음과 같은 경우에 Overfitting이 발생할 수 있다.

1. 모델이 너무 복잡할 때 (과도한 다항식 차수 등)
2. 특성(Feature)의 수가 데이터 수에 비해 많을 때
3. 노이즈를 그대로 학습할 때

이러한 Overfitting을 완화하기 위한 대표적인 방법은 다음과 같다.

1. 정규화
   - Ridge Regression, Lasso Regression
2. 모델 단순화
   - 너무 복잡한 다항식 모델을 사용하는 대신 차수를 낮추어 단순한 모델을 사용
3. 더 많은 데이터 확보
   - 학습 데이터가 많을수록 모델은 일반화에 유리해지며, Overfiting 위험이 줄어든다.
4. 교차 검증(Cross-validation)
   - 모델을 검증 데이터에서 평가하여 일반화 성능이 좋은 모델 구조나 하이퍼 파라미터를 선택

### 2. Ridge Regression의 개념을 설명하고, 일반적인 선형 회귀와의 차이점을 서술하시오. 정규화 항이 모델에 어떤 영향을 주는지 설명하시오

Ridge Regression은 선형 회귀에서 과적합(Overfitting) 문제를 방지하기 위해 도입된 정규화 기법 중 하나이다. 일반적인 선형 회귀는 잔차제곱합(RSS)을 최소화하는 것이 목적이지만, Ridge Regression은 여기에 가중치의 크기를 제한하는 정규화 항을 추가하여 모델이 과도하게 복잡해지는 것을 억제한다.

$$L_{\text{Ridge}} = \text{RSS} + \lambda w^Tw$$

### 3. Ridge Regression의 해석적 해(closed-form solution)를 유도하시오. 정규 방정식(Normal Equation)과 비교하여 Ridge 정규화 항이 어떻게 반영되는지 수식과 함께 설명하시오

$$L_{\text{Ridge}} = ||y - Hw||^2 + \lambda ||w||^2_2 = (y - Hw)^T(y - Hw) + \lambda w^Tw$$
$$ = y^Ty - 2w^TH^Ty + w^TH^THw + \lambda w^Tw$$

$$\nabla L_{\text{Ridge}} = -2H^Ty +2H^THw + 2\lambda w = 0$$
$$ (H^TH + \lambda I)w = H^Ty$$
$$ \hat{w} = (H^TH + \lambda I)^{-1}H^Ty$$

정규화 항 $\lambda I$가 추가 되어서 $H^TH$가 역행렬이 존재하지 않더라도 full-rank를 만들어서 수치적으로 더 안정적인 해를 얻을 수 있다.

### 4. Ridge Regression과 Lasso Regression의 정규화 방식의 차이를 설명하시오. 각 기법이 모델에 미치는 영향과 변수 선택(feature selection) 측면에서의 차이도 서술하시오

1. 정규화 방식 차이
   - Ridge는 L2 정규화를 사용하여 계수들의 크기를 줄이는 방향으로 제약을 준다. 하지만 계수들이 0이 되지는 않는다.
   - Lasso는 L1 정규화를 사용하며, 일부 계수를 완전히 0으로 만들어 변수 선택(Feature Selection) 효과를 낸다.
2. 모델 해석력 및 희소성(Sparsity)
   - Ridge는 모든 변수를 남기므로, 변수 선택은 하지 않지만, 과적합을 완화해준다.
   - Lasso는 계수를 0으로 만드는 특성 덕분에 모델을 간단하게 만들고 해석력을 높일 수 있다.
3. 사용 시점 차이
   - Ridge는 많은 상관관계가 있는 변수들이 있는 경우에 유리하다.
   - Lasso는 불필요한 변수를 제거하여 간결한 모델이 필요한 경우에 적합하다.

결론적으로, 두 정규화 방법은 모두 모델의 복잡도를 줄이지만, Ridge는 모든 특성을 남겨두고 계수를 작게 만들며, Lasso는 불필요한 특성을 제거하는 변수 선택 효과를 가진다.

### 5. λ의 값이 클 때와 작을 때 모델에 어떤 영향을 주는지 서술하고, 적절한 λ를 선택하는 일반적인 방법도 함께 설명하시오

1. λ가 작을 때:
   - 정규화 효과가 거의 없어서 모델이 원래의 선형 회귀와 비슷해진다. 모델이 학습 데이터에 과도하게 적합(Overfitting)될 수 있다. 계수들이 자유롭게 커질 수 있다.

1. λ가 클 때:
   - 정규화 효과가 강해져서 모델이 단순해지고 일반화 성능이 좋아질 수 있다.
   - 계수들의 크기를 줄이며, Lasso의 경우에는 일부 계수를 0으로 만들어 변수 선택 효과도 발생한다.
   - 하지만 너무 크면 **과소적합(Underfitting)**이 발생할 수 있다.
1. 적절한 λ 선택 방법:
   - 일반적으로 **교차 검증(Cross-validation)**을 통해 λ 값을 실험적으로 선택한다.
   - 예를 들어, 여러 후보 λ 값들에 대해 모델을 학습하고, 검증 성능이 가장 좋은 값을 선택한다.

결론적으로, λ는 모델의 복잡도를 조절하는 핵심 하이퍼파라미터로, Overfitting과 Underfitting 사이의 균형을 맞추는 데 중요한 역할을 하며, 실전에서는 교차 검증을 통해 적절한 값을 선택하는 것이 일반적이다.

### 6. validation set과 cross validation의 차이점

✅ Validation Set

학습 데이터를 **훈련(train)**과 **검증(validation)**으로 한 번 분할한다.
모델은 훈련 데이터로 학습되고, 검증 데이터로 성능을 평가한다.
장점: 구현이 간단하고 계산 비용이 낮다.
단점: 검증 세트의 분할 방식에 따라 성능 평가가 불안정하거나 편향될 수 있음.

✅ Cross-Validation (예: K-fold Cross-Validation)

학습 데이터를 K개의 폴드로 나눈 후, K번 반복 학습한다.
각 반복마다 하나의 폴드를 검증용으로 사용하고 나머지 K-1개는 학습에 사용한다.
각 폴드의 성능을 평균내어 최종 평가 점수를 구한다.
장점: 평가 결과가 더 안정적이고 일반화된 성능 추정 가능.
단점: 여러 번 학습하므로 계산 비용이 높다.

| 항목 | Validation Set | Cross-Validation|
|---|---|---|
| 분할 횟수 | 1회 | 여러 번 (K회)|
| 안정성 | 낮음 (편향 가능) | 높음 (분산 낮음)|
| 계산 비용 | 낮음 | 높음|
| 사용 용도 | 빠른 실험, 대략적 튜닝 | 정밀한 하이퍼파라미터 튜닝, 모델 선택|

## Neural networks

### 1. Perceptron의 한계와 이를 해결하기 위한 방법에 대해 서술하시오

Perceptron은 가장 단순한 형태의 인공신경망으로, 입력값의 선형 결합에 대해 일정 임계값을 기준으로 출력을 결정한다.\
하지만 XOR 문제처럼 선형 분리가 불가능한 문제를 해결할 수 없다는 근본적인 한계를 가진다.\
이러한 문제는 하나의 선형 결정 경계로 분류할 수 없는 입력 데이터셋에서 발생한다.

이러한 한계를 해결하기 위해 다층 퍼셉트론(Multilayer Perceptron, MLP)이 제안되었다.\
MLP는 여러 개의 은닉층(hidden layer)을 가지며, 각 층에서 비선형 활성함수(예: sigmoid, ReLU 등)를 적용함으로써 비선형 결정경계를 학습할 수 있다.\
또한, 오차역전파(Backpropagation) 알고리즘을 사용해 학습을 수행하며, 이를 통해 비선형적인 문제도 효과적으로 해결할 수 있다.

### 2. Sigmoid 함수를 출력층에서 사용하는 신경망에서의 장단점에 대해 서술하시오

Sigmoid 함수는 출력 범위를 (0,1)로 제한하는 S자 형태의 비선형 함수로, 출력값을 활률처럼 해석할 수 있어 이진 분류 문제에서 자주 사용된다.

장점:

1. 출력이 확률로 해석 가능하여 이진 분류에 적합하다.
2. 역속적이고 미분 가능하여 경사하강법 기반의 학습이 가능하다.

단점:

1. Vanishing Gradient 문제: 입력값이 매우 크거나 작을경우 기울기가 0에 가까워져 학습이 느려지거나 멈춘다.
2. 출력의 중심이 0이 아님: Sigmoid의 출력 범위는 0과 1사이로, 평균 출력이 0보다 크기 때문에 다음 층의 gradient가 평향될 수 있다.

### 3. Sigmoid, Hyperbolic Tangent, Rectified Linear Unit(ReLU) 함수와 미분식을 작성하시오

#### Sigmoid Function

$$g(z) = \frac{1}{1 + e^{-z}}$$
$$g'(z) = g(z)(1 - g(z))$$

#### Hyperbolic Tangent

$$ g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$$
$$g'(z) = 1 - g(z)^2$$

#### Rectified Linear Unit

$$g(z) = max(0, z)$$
$$g'(z) =\begin{cases}
1 & z > 0 \\
0 & \text{otherwise}
\end{cases}$$

### 4. Full Batch, Mini-Batch, Stochastic Gradient Descent의 차이점을 개념, 계산 효율, 수렴 특성의 측면에서 비교하여 서술하시오.

Gradient Descent는 손실 함수의 기울기를 계산해 파라미터를 최적화하는 알고리즘으로, 사용하는 데이터 단위에 따라 Full Batch, Mini-Batch, Stochatic Gradient Descent로 구분한다.

#### Full Batch Gradient Descent

전체 데이터를 사용해 기울기를 계산한다.

장점: 각 업데이트가 전체 데이터를 반영하므로 안정적이고 수렴 경로가 매끄럽다.\
단점: 데이터가 많을 경우 한번의 업데이트에도 계산량이 매우 크로 메모리 소모가 많아 비효율적이다.

#### Stochastic Gradient Descent

무작위로 선택된 단일 샘플에 대해 기울기를 계산한다.

장점: 매우 빠르고 메모리 효율이 좋다. 노이즈가 지역 최소값에서 빠져나오는데 유리하다.\
단점: 수렴 경로가 불안정하고, 진동이 심해 최적점 부근에서 잘 수렴하지 못할 수 있다.

#### Mini-Batch Gradient Descent

데이터를 여러 개의 소규모 그룹으로 나눈후 각 배치 단위로 기울기를 계산한다.

장점: Full Batch보다 빠르고, SGD보다 수렴 안정성이 뛰어나며 병렬 연산에 유리하다.\
단점: 배치 크기에 따라 수렴 속도와 안정성의 균형이 달라져 적절한 튜닝이 필요하다.

결론적으로, Mini-Batch는 계산 효율성과 수렴 안정성 간의 균형점으로 널리 사용되며,\
데이터 크기와 모델 특성에 따라 적절한 방법을 선택해야한다.

### 5. Backpropagation 계산문제.

## Convolutional neural networks

### 1. Shallow Learning과 Deep Learning의 개념과 차이점을 구조, 표현력, 학습 방식의 측면에서 비교하여 서술하시오.

Shallow Learning은 일반적으로 입력층과 출력층 사이에 은닉층이 하나만 존재하는 학습모델을 의미한다.\
대표적인 예로는 SVM, 로지스틱 회귀, 결정 트리 등이 있다.\
이들은 특징을 수동으로 추출한 후 모델이 학습하는 방식이며, 특징 엔지니어링의 성능에 크게 의존한다.

반면, Deep Learning은 여러 개의 은닉층을 포함한 신경망 구조로, 입력 데이터로부터 계층적으로 특징을 자동으로 추출하고 학습한다.\
대표적인 모델로는 CNN, RNN, Transformer등이 있으며, 이미지, 음성, 자연어 처리 등에서 매우 뛰어난 성능을 보인다.

### 2. Fully Connected Neural Network와 Convolutional Neural Network의 구조적 차이와 각각의 장단점을 서술하시오.

- Fully Connected Neural Network는 각 뉴런이 이전 층의 모든 뉴런과 연결되어 있는 구조로, 입력 데이터의 구조적 특성을 고려하지 않고 모든 특징을 동일하게 취급한다.\
 이때문에 입력 데이터를 1차원 벡터로 펼쳐야 하며, 위치 정보나 지역적 패턴을 학습하기 어렵다.

- Convolutional Neural Network는 이미지와 같은 2차원 데이터의 공간적 구조를 유지하며, 필터를 통해 지역적인 특징을 추출한다.\
각 필터는 이미지의 일부분에만 적용되어 파라미터 수가 줄어들고, 지역적 패턴에 대한 학습이 가능하다.

### 3. CNN에서 Convolution Layer, Pooling Layer, Fully Connected Layer의 역할과 차이점을 서술하시오.

- Conv Layer
  - 입력 데이터의 일부분에 필터(또는 커널)를 적용하여 지역적 특징을 추출하는 역할을 한다.
  - 파라미터 공유와 지역 연결성을 통해 파라미터 수를 줄이고, 같은 특징이 여러 위치에서 반복되는 패턴을 효과적으로 학습
- Pooling Layer
  - Conv Layer에서 추출한 특징맵의 크기를 줄이는 다운샘플링 작업을 수행한다.
  - 주로 Max Pooling이나 Average Pooling을 사용
  - 공간정보를 요약해 계산량을 줄이고 과적합을 방지하는데 도음을 준다.
- Fully Connected Layer
  - CNN의 마지막 부분에서 사용되며, 추출된 고차원 특징들을 1차원 벡터로 변환한 뒤 최종 분류나 회귀 작업에 사용된다.
  - 이는 전통적인 MLP 구조와 같으며, 각 뉴런이 이전 층의 모든 뉴런과 연결된다.
  - 이를 통해 전역적인 판단(global reasoning)을 수행할 수 있다.

### 4. Conv layer 계산문제

Q. 다음 조건에서 파라미터 수와 출력 크기를 계산하시오:

입력: 64×64×1 (흑백 이미지)

Conv 레이어:
- 필터: 3×3
- 출력 채널: 32개
- 스트라이드: 2
- 패딩: 1

```txt
정답:
- 출력 크기: 32×32×32
- 파라피터 수: 320
```

### 5. Pooling layer 계산문제

Q. 다음 조건에서 파라미터 수와 출력 크기를 계산하시오:

입력 이미지: 28×28×64 (높이×너비×채널)

Max Pooling:
- 커널 크기: 2×2
- 스트라이드(stride): 2
- 패딩(padding): 없음

```txt
정답:
- 출력 크기: 14x14x64
- 파라피터 수: 0
```
